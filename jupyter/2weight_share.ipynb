{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MXNet package\n",
    "from mxnet import nd, init, cpu, gpu, gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon.data.vision import CIFAR10, transforms as T\n",
    "from gluoncv.model_zoo import cifar_resnet56_v1\n",
    "\n",
    "# Normal package\n",
    "import time\n",
    "import types\n",
    "import logging\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom package\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from compress.quantize import CodebookQuantizer\n",
    "from compress.utils import collect_conv_and_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # Model\n",
    "    num_class = 10\n",
    "    \n",
    "    # Train\n",
    "    max_steps = 30000\n",
    "    train_batch_size = 128\n",
    "    val_batch_size = 256\n",
    "    train_num_workers = 4\n",
    "    val_num_workers = 4\n",
    "    lr = 1e-6\n",
    "    \n",
    "    # Record\n",
    "    ckpt_dir = \"./checkpoints\"\n",
    "    main_tag = 'cifar10_resnet56_v1_CBQuantize'\n",
    "    ckpt_prefix = 'cifa10_resnet_56_v1_CBQuantize'\n",
    "    train_record_per_steps = 50\n",
    "    val_per_steps = 100\n",
    "    spotter_starts_at = 10000\n",
    "    spotter_window_size = 10\n",
    "    patience = 20\n",
    "    snapshot_per_steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(Config.ckpt_dir):\n",
    "    os.mkdir(Config.ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_stamp = time.strftime('%Y%m%d_%H%M%S',time.localtime(time.time()))\n",
    "writer = SummaryWriter(log_dir=\"runs/{}_{}\".format(Config.main_tag, datetime_stamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cifar_resnet56_v1(pretrained=True)\n",
    "net.load_parameters(\"./checkpoints/cifa10_resnet_56_v1_conv0.3_fc1.5-018000.params\", ignore_extra=True)   # 91.9%, ignore mask\n",
    "\n",
    "def merge_bn(conv, bn):\n",
    "    gamma = bn.gamma.data()\n",
    "    beta = bn.beta.data()\n",
    "    mean = bn.running_mean.data()\n",
    "    var = bn.running_var.data()\n",
    "    \n",
    "    weight = conv.weight.data()\n",
    "    w_shape = conv.weight.shape\n",
    "    cout = w_shape[0]\n",
    "    conv.weight.set_data( (weight.reshape(cout, -1) * gamma.reshape(-1, 1) \\\n",
    "                                  / nd.sqrt(var + 1e-5).reshape(-1, 1)).reshape(w_shape) )\n",
    "    if conv.bias is None:\n",
    "        conv._kwargs['no_bias'] = False\n",
    "        conv.bias = conv.params.get('bias',\n",
    "                                    shape=(cout,), init=\"zeros\",\n",
    "                                    allow_deferred_init=True)\n",
    "        conv.bias.initialize()\n",
    "    bias = conv.bias.data()\n",
    "    conv.bias.set_data(gamma * (bias - mean) / nd.sqrt(var + 1e-5) + beta)\n",
    "    \n",
    "    def _forward(self, F, x, *args, **kwargs):\n",
    "        return x\n",
    "    bn.hybrid_forward = types.MethodType(_forward, bn)\n",
    "\n",
    "conv_bn = [(net.features[2][i].body[0], net.features[2][i].body[1]) for i in range(9)]\n",
    "conv_bn.extend([(net.features[2][i].body[3], net.features[2][i].body[4]) for i in range(9)])\n",
    "conv_bn.append((net.features[0], net.features[1]))\n",
    "for conv, bn in conv_bn:\n",
    "    merge_bn(conv, bn)\n",
    "\n",
    "net.collect_params().reset_ctx(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantizer = CodebookQuantizer(net, exclude=[net.features[0]], default_bits=3)\n",
    "quantizer.apply_quantize(set_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, num_class, dataloader, ctx):\n",
    "    t = time.time()\n",
    "    correct_counter = nd.zeros(num_class)\n",
    "    label_counter = nd.zeros(num_class)\n",
    "    test_num_correct = 0\n",
    "    eval_loss = 0.\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "\n",
    "        outputs = net(X)\n",
    "        loss = loss_func(outputs, y)\n",
    "        eval_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        test_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "\n",
    "        pred = pred.as_in_context(cpu())\n",
    "        y = y.astype('float32').as_in_context(cpu())\n",
    "        for p, gt in zip(pred, y):\n",
    "            label_counter[gt] += 1\n",
    "            if p == gt:\n",
    "                correct_counter[gt] += 1\n",
    "\n",
    "    eval_loss /= len(test_dataset)\n",
    "    eval_acc = test_num_correct / len(test_dataset)\n",
    "    eval_acc_avg = (correct_counter / (label_counter+1e-10)).mean().asscalar()\n",
    "    \n",
    "    return eval_loss, eval_acc, eval_acc_avg, time.time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no augmentation\n",
    "train_transformer = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "eval_transformer = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(train=True).transform_first(train_transformer)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=Config.train_batch_size, shuffle=True, \n",
    "                          num_workers=Config.train_num_workers, last_batch='discard')\n",
    "test_dataset = CIFAR10(train=False).transform_first(train_transformer)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=Config.val_batch_size, shuffle=False, \n",
    "                          num_workers=Config.val_num_workers, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(train_loader)\n",
    "print(f'trainset size => {train_size}')\n",
    "print(f'valset size => {val_size}')\n",
    "steps_per_epoch = train_size / Config.train_batch_size\n",
    "print(f'{steps_per_epoch} steps for per epoch (BATCH_SIZE={Config.train_batch_size})')\n",
    "print(\"record per {} steps ({} samples, {} times per epoch)\".format(\n",
    "                                                            Config.train_record_per_steps,\n",
    "                                                            Config.train_record_per_steps * Config.train_batch_size,\n",
    "                                                            steps_per_epoch / Config.train_record_per_steps))\n",
    "print(\"evaluate per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.val_per_steps,\n",
    "                                                    steps_per_epoch / Config.val_per_steps))\n",
    "print(\"spotter start at {} steps ({} epoches)\".format(\n",
    "                                                Config.spotter_starts_at,\n",
    "                                                Config.spotter_starts_at / steps_per_epoch))\n",
    "print(\"size of spotter window is {} ({} steps)\".format(\n",
    "                                                Config.spotter_window_size,\n",
    "                                                Config.spotter_window_size * Config.val_per_steps))\n",
    "print(\"max patience: {} ({} steps; {} samples; {} epoches)\".format(\n",
    "                                                            Config.patience,\n",
    "                                                            Config.patience * Config.val_per_steps,\n",
    "                                                            Config.patience * Config.val_per_steps * Config.train_batch_size,\n",
    "                                                            Config.patience * Config.val_per_steps / steps_per_epoch))\n",
    "print(\"snapshot per {} steps ({} times per epoch)\".format(\n",
    "                                                    Config.snapshot_per_steps,\n",
    "                                                    steps_per_epoch / Config.snapshot_per_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steps = 0\n",
    "good_acc_window = [0.] * Config.spotter_window_size\n",
    "estop_loss_window = [0.] * Config.patience\n",
    "quantize_input_offline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': Config.lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First evaluate\n",
    "eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, global_steps)\n",
    "writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "    'val': eval_acc,\n",
    "    'val_avg': eval_acc_avg\n",
    "}, global_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "size_per_record = Config.train_record_per_steps * Config.train_batch_size\n",
    "flag_early_stop = False\n",
    "train_loss = 0.\n",
    "train_num_correct = 0\n",
    "prune_counter = 0\n",
    "t = time.time()\n",
    "while global_steps < Config.max_steps and not flag_early_stop:\n",
    "    for X, y in train_loader:\n",
    "        # Move data to gpu\n",
    "        X = X.as_in_context(gpu(0))\n",
    "        y = y.as_in_context(gpu(0))\n",
    "        # Forward & Backward\n",
    "        with autograd.record():\n",
    "            outputs = net(X)\n",
    "            loss = loss_func(outputs, y)\n",
    "        loss.backward()\n",
    "#         trainer.step(Config.train_batch_size)\n",
    "        quantizer.update(Config.lr)\n",
    "        net.collect_params().zero_grad()\n",
    "        \n",
    "        train_loss += loss.sum().asscalar()\n",
    "        pred = outputs.argmax(axis=1)\n",
    "        train_num_correct += (pred == y.astype('float32')).sum().asscalar()\n",
    "        \n",
    "        # Record training info\n",
    "        if global_steps and global_steps % Config.train_record_per_steps == 0:\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'train': train_loss/size_per_record}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {'train': train_num_correct/size_per_record}, global_steps)\n",
    "            train_loss = 0.\n",
    "            train_num_correct = 0\n",
    "            \n",
    "        # Evaluate\n",
    "        if global_steps and global_steps % Config.val_per_steps == 0:\n",
    "            # Evaluate\n",
    "            eval_loss, eval_acc, eval_acc_avg, __ = evaluate(net, Config.num_class, test_loader, ctx=gpu(0))\n",
    "            writer.add_scalar(f'{Config.main_tag}/Speed', Config.val_per_steps / (time.time() - t), global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Loss', {'val': eval_loss}, global_steps)\n",
    "            writer.add_scalars(f'{Config.main_tag}/Acc', {\n",
    "                'val': eval_acc,\n",
    "                'val_avg': eval_acc_avg\n",
    "            }, global_steps)\n",
    "            \n",
    "#             # Spotter\n",
    "#             good_acc_window.pop(0)\n",
    "#             if global_steps >= Config.spotter_starts_at and eval_acc > max(good_acc_window):\n",
    "#                 print( \"catch a good model with acc {:.6f} at {} step\".format(eval_acc, global_steps) )\n",
    "#                 writer.add_text(Config.main_tag, \"catch a good model with acc {:.6f}\".format(eval_acc), global_steps)\n",
    "#                 net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))\n",
    "#             good_acc_window.append(eval_acc)\n",
    "\n",
    "#             # Early stop\n",
    "#             estop_loss_window.pop(0)\n",
    "#             estop_loss_window.append(eval_loss)\n",
    "#             if global_steps > Config.val_per_steps * len(estop_loss_window):\n",
    "#                 min_index = estop_loss_window.index( min(estop_loss_window) )\n",
    "#                 writer.add_scalar(f'{Config.main_tag}/val/Patience', min_index, global_steps)\n",
    "#                 if min_index == 0:\n",
    "#                     flag_early_stop = True\n",
    "#                     print(\"early stop at {} steps\".format(global_steps))\n",
    "#                     break\n",
    "            \n",
    "            t = time.time()\n",
    "        \n",
    "        # Snapshot\n",
    "        if global_steps and global_steps % Config.snapshot_per_steps == 0:\n",
    "            net.save_parameters(\"{}/{}-{:06d}.params\".format(Config.ckpt_dir, Config.ckpt_prefix, global_steps))\n",
    "            \n",
    "        # Next step\n",
    "        global_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
